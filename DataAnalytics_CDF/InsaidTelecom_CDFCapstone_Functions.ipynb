{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InsaidTelecom-CDFCapstone_Functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNN232/aPbRIsO/Sj3ji8+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhoshbvsrk/CapstoneProjects/blob/main/DataAnalytics_CDF/InsaidTelecom_CDFCapstone_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwbT46AUJXEb"
      },
      "source": [
        "def load_datasets_from_drive():\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount ('/content/gdrive')\n",
        "\n",
        "  !unzip gdrive/My\\ Drive/CDF_Capstone_Project/events_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHrx7o7yW3WX"
      },
      "source": [
        "def read_csv_file():\n",
        "  import pandas as pd \n",
        "  import numpy as np\n",
        "\n",
        "  pd.set_option('display.float_format', lambda x: '%18f' %x)\n",
        "\n",
        "  events_data_df=pd.read_csv(\"events_data.csv\")\n",
        "\n",
        "  return events_data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2XfPbnUKyyg"
      },
      "source": [
        "def read_dataset_genderage():\n",
        "\n",
        "  import pandas as pd \n",
        "  import numpy as np\n",
        "\n",
        "  !pip install mysql-connector-python-rf\n",
        "\n",
        "  import mysql.connector\n",
        "  \n",
        "#this is to connect to mysql database\n",
        "  mydb = mysql.connector.connect(\n",
        "      host=\"cpanel.insaid.co\",\n",
        "      user=\"student\",\n",
        "      password=\"student\",\n",
        "      database=\"Capstone1\"\n",
        "    )\n",
        "\n",
        "#declaring a cursor variable of database to hold data of a specific table\n",
        "  mycursor = mydb.cursor()\n",
        "\n",
        "#mycursor variable holds the entire data from gender_age_train table\n",
        "  mycursor.execute(\"SELECT * FROM gender_age_train\")\n",
        "\n",
        "#using fetchall function to get data from cursor and populating in another variable\n",
        "  myresult = mycursor.fetchall()\n",
        "\n",
        "#inserting table data from variable into a dataframe\n",
        "  gender_age_train_df = pd.DataFrame(myresult,columns=['device_id','Gender','Age','Age_Group'])\n",
        "  return gender_age_train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKf_tEgUewWT"
      },
      "source": [
        "def read_dataset_phonebrandmodel():\n",
        "\n",
        "  import pandas as pd \n",
        "  import numpy as np\n",
        "  import mysql.connector\n",
        "  \n",
        "#this is to connect to mysql database\n",
        "  mydb = mysql.connector.connect(\n",
        "      host=\"cpanel.insaid.co\",\n",
        "      user=\"student\",\n",
        "      password=\"student\",\n",
        "      database=\"Capstone1\"\n",
        "    )\n",
        "\n",
        "#declaring a cursor variable of database to hold data of a specific table\n",
        "  mycursor = mydb.cursor()\n",
        "\n",
        "#mycursor variable holds the entire data from phone_brand_device_model table\n",
        "  mycursor.execute(\"SELECT * FROM phone_brand_device_model\")\n",
        "\n",
        "#using fetchall function to get data from cursor and populating in another variable, this overwrites data from previous load\n",
        "  myresult = mycursor.fetchall()\n",
        "\n",
        "#inserting table data from variable into a dataframe\n",
        "  phone_brand_device_model_df = pd.DataFrame(myresult,columns=['device_id','Brand','Model'])\n",
        "  return phone_brand_device_model_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Di18-WFdEPP"
      },
      "source": [
        "def preprocessing_checks(events_data_df,gender_age_train_df,phone_brand_device_model_df):\n",
        "  print('Datasets info is as follows:')\n",
        "  print(\"\\n Events Data Set info is:\",events_data_df.info())\n",
        "  print('\\n Gender Age Data Set info is:\\n',gender_age_train_df.info())\n",
        "  print('\\n Mobile Brand Model Data Set info is:\\n',phone_brand_device_model_df.info())\n",
        "  print(\"Missing values across 3 datasets are:\")\n",
        "  print(\"\\n Missing values in events_data_df is:\\n\",events_data_df.isnull().sum())\n",
        "  print(\"\\n Missing values in gender_age_train_df is:\\n\",gender_age_train_df.isnull().sum())\n",
        "  print(\"\\n Missing values in phone_brand_device_model_df is:\\n\",phone_brand_device_model_df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0tNm9zWiWBF"
      },
      "source": [
        "def preprocessing(events_data_df,phone_brand_device_model_df):\n",
        "  print(\"\\n Missing Values Before Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #Preparing list of unique latitude values for the missing deviceid records.\n",
        "  latlon_df = events_data_df[events_data_df.device_id.isnull()]['latitude'].unique()\n",
        "  #For imputing Missing values in DeviceId, we've fetched unique latitude for the missing deviceid and found that for that latitude there is only one deviceid throughout the dataset.\n",
        "  #Hence, we'll be imputing missing deviceid values by replacing the deviceid which the same latitudeid shares\n",
        "  for latitude in latlon_df:\n",
        "    devid = list(events_data_df[(events_data_df.latitude == latitude) & (events_data_df.device_id.notnull())].device_id)[0]\n",
        "    #print('Latitude is {} for DeviceID: {}'.format(devid,latitude))\n",
        "    events_data_df.loc[events_data_df.latitude == latitude, 'device_id'] = devid\n",
        "  #Missing Values post imputing DeviceID\n",
        "  print(\"\\n Missing Values After DeviceID Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #For imputing the missing values in State, we'll first find all their respecitve city values\n",
        "  city_list = list(events_data_df[events_data_df.state.isnull()].city.unique())\n",
        "  #Will loop over the above cities list, fetch it's relevant state value where state value is not null and then populate that to all the records where city value is matching with the city value from our list\n",
        "  for city in city_list:\n",
        "    state = list(events_data_df[(events_data_df.city == city) & (events_data_df.state.notnull())].state)[0]\n",
        "    #print(\"State is {} for City {}\".format(state,city))\n",
        "    events_data_df.loc[events_data_df.city == city, 'state'] = state\n",
        "  #Missing Values post imputing State\n",
        "  print(\"\\n Missing Values After State Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #checking if the device_id is different for the records where latitude & longitude are null\n",
        "  dev_id = events_data_df[(events_data_df.latitude.isnull()) & (events_data_df.longitude.isnull())].sort_values(by=['device_id'])['device_id'].unique()\n",
        "  #Imputing missing values in latitude & longitude\n",
        "  #From the previous steps we found that there are 21 device_ids for the missing latitude & longitude records.\n",
        "  #For these 21 device_ids there are unique latitude & longitude combination. Hence, we'll be replacing the missing latitude & longitude values for these devices with the same latitude & longitude values\n",
        "  for i in dev_id:\n",
        "    lat = list(events_data_df[(events_data_df.device_id == i) & (events_data_df.latitude.notnull())].latitude)[0]\n",
        "    lon = list(events_data_df[(events_data_df.device_id == i) & (events_data_df.latitude.notnull())].longitude)[0]\n",
        "    city = list(events_data_df[events_data_df.device_id == i].city)[0]\n",
        "    state = list(events_data_df[events_data_df.device_id == i].state)[0]\n",
        "    #print('For DeviceID {} Latitude is: {}, Longitude is: {}, city is: {} & State is: {}'.format(i,lat,lon,city,state))\n",
        "    events_data_df.loc[events_data_df.device_id == i, 'latitude'] = lat\n",
        "    events_data_df.loc[events_data_df.device_id == i, 'longitude'] = lon\n",
        " #Missing Values post imputing Latitude & Longitude\n",
        "  print(\"\\n Missing Values After Latitude & Longitude Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #To replace Non-English Characters in Brand column of phone_brand_device_model dataframe\n",
        "  #Add a brand name per brand in Chinese for which we don´t have a translation\n",
        "  phone_brands_mapping = {\"三星\": \"samsung\",\"天语\": \"Ktouch\", \"海信\": \"hisense\", \"联想\": \"lenovo\", \"欧比\": \"obi\",\n",
        "                                \"爱派尔\": \"ipair\", \"努比亚\": \"nubia\", \"优米\": \"youmi\", \"朵唯\": \"dowe\", \"黑米\": \"heymi\",\n",
        "                                \"锤子\": \"hammer\", \"酷比魔方\": \"koobee\", \"美图\": \"meitu\", \"尼比鲁\": \"nibilu\", \"一加\": \"oneplus\",\n",
        "                                \"优购\": \"yougo\", \"诺基亚\": \"nokia\", \"糖葫芦\": \"candy\", \"中国移动\": \"ccmc\", \"语信\": \"yuxin\",\n",
        "                                \"基伍\": \"kiwu\", \"青橙\": \"greeno\", \"华硕\": \"asus\", \"夏新\": \"panasonic\", \"维图\": \"weitu\",\n",
        "                                \"艾优尼\": \"aiyouni\", \"摩托罗拉\": \"moto\", \"乡米\": \"xiangmi\", \"米奇\": \"micky\", \"大可乐\": \"bigcola\",\n",
        "                                \"沃普丰\": \"wpf\", \"神舟\": \"hasse\", \"摩乐\": \"mole\", \"飞秒\": \"fs\", \"米歌\": \"mige\", \"富可视\": \"fks\",\n",
        "                                \"德赛\": \"desci\", \"梦米\": \"mengmi\", \"乐视\": \"lshi\", \"小杨树\": \"smallt\", \"纽曼\": \"newman\",\n",
        "                                \"邦华\": \"banghua\", \"E派\": \"epai\", \"易派\": \"epai\", \"普耐尔\": \"pner\", \"欧新\": \"ouxin\", \"西米\": \"ximi\",\n",
        "                                \"海尔\": \"haier\", \"波导\": \"bodao\", \"糯米\": \"nuomi\", \"唯米\": \"weimi\", \"酷珀\": \"kupo\", \"谷歌\": \"google\",\n",
        "                                \"昂达\": \"ada\", \"聆韵\": \"lingyun\", \"小米\": \"Xiaomi\", \"华为\": \"Huawei\", \"魅族\": \"Meizu\", \"中兴\": \"ZTE\",\n",
        "                                \"酷派\": \"Coolpad\", \"金立\": \"Gionee\", \"SUGAR\": \"SUGAR\", \"OPPO\": \"OPPO\", \"vivo\": \"vivo\", \"HTC\": \"HTC\",\n",
        "                                \"LG\": \"LG\", \"ZUK\": \"ZUK\", \"TCL\": \"TCL\", \"LOGO\": \"LOGO\", \"SUGAR\": \"SUGAR\", \"Lovme\": \"Lovme\",\n",
        "                                \"PPTV\": \"PPTV\", \"ZOYE\": \"ZOYE\", \"MIL\": \"MIL\", \"索尼\" : \"Sony\", \"欧博信\" : \"Opssom\", \"奇酷\" : \"Qiku\",\n",
        "                                \"酷比\" : \"CUBE\", \"康佳\" : \"Konka\", \"亿通\" : \"Yitong\", \"金星数码\" : \"JXD\", \"至尊宝\" : \"Monkey King\",\n",
        "                                \"百立丰\" : \"Hundred Li Feng\", \"贝尔丰\" : \"Bifer\", \"百加\" : \"Bacardi\", \"诺亚信\" : \"Noain\", \n",
        "                                \"广信\" : \"Kingsun\", \"世纪天元\" : \"Ctyon\", \"青葱\" : \"Cong\", \"果米\" : \"Taobao\", \"斐讯\" : \"Phicomm\",\n",
        "                                \"长虹\" : \"Changhong\", \"欧奇\" : \"Oukimobile\", \"先锋\" : \"XFPLAY\", \"台电\" : \"Teclast\", \"大Q\" : \"Daq\",\n",
        "                                \"蓝魔\" : \"Ramos\", \"奥克斯\" : \"AUX\", \"索尼\" : \"Sony\", \"欧博信\" : \"Opssom\", \"奇酷\" : \"Qiku\",\n",
        "                                \"酷比\" : \"CUBE\", \"康佳\" : \"Konka\", \"亿通\" : \"Yitong\", \"金星数码\" : \"JXD\", \"至尊宝\" : \"Monkey King\",\n",
        "                                \"百立丰\" : \"Hundred Li Feng\", \"贝尔丰\" : \"Bifer\", \"百加\" : \"Bacardi\", \"诺亚信\" : \"Noain\",\n",
        "                                \"广信\" : \"Kingsun\", \"世纪天元\" : \"Ctyon\", \"青葱\" : \"Cong\", \"果米\" : \"Taobao\", \"斐讯\" : \"Phicomm\",\n",
        "                                \"长虹\" : \"Changhong\", \"欧奇\" : \"Oukimobile\", \"先锋\" : \"XFPLAY\", \"台电\" : \"Teclast\", \"大Q\" : \"Daq\", \n",
        "                                \"蓝魔\" : \"Ramos\", \"奥克斯\" : \"AUX\", \"飞利浦\": \"Philips\", \"智镁\": \"Zhimei\", \"惠普\": \"HP\",\n",
        "                                \"原点\": \"Origin\", \"戴尔\": \"Dell\", \"碟米\": \"Diemi\", \"西门子\": \"Siemens\", \"亚马逊\": \"Amazon\",\n",
        "                                \"宏碁\": \"Acer\",\n",
        "                                '世纪星': \"UB1\", '丰米': \"UB2\", '优语':'UB3', '凯利通': \"UB4\", '唯比': \"UB5\", '嘉源': \"UB6\",\n",
        "                                 '大显': \"UB7\", '天宏时代': \"UB8\", '宝捷讯': 'UB9','帷幄': 'UB10', '德卡诺': 'UB11',\n",
        "                                '恒宇丰': 'UB12', '本为': 'UB13', '极米': 'UB14', '欧乐迪': 'UB15', '欧乐酷': 'UB16',\n",
        "                                '欧沃': 'UB17', '瑞米': 'UB18', '瑞高': 'UB19', '白米': 'UB20', '虾米': 'UB21', '赛博宇华': 'UB22',\n",
        "                                '首云': 'UB23', '鲜米': 'UB24','E人E本':'eben'}\n",
        "\n",
        "  phone_brand_device_model_df.Brand = phone_brand_device_model_df.Brand.apply(lambda x: phone_brands_mapping.setdefault(x.strip(),x.strip()))\n",
        "  print(\"\\n After mapping Non-English Characters in Brand: \\n\",phone_brand_device_model_df.Brand.unique())\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Tfz4RNnkgU"
      },
      "source": [
        "def filter_data(events_data_df):\n",
        "  #creating a dataframe which contains event data for only 6 states under consideration\n",
        "  events_data_filtered_df= events_data_df[(events_data_df.state == 'WestBengal') | (events_data_df.state == 'Karnataka') | (events_data_df.state == 'Gujarat') | (events_data_df.state == 'Bihar') | (events_data_df.state == 'Punjab') | (events_data_df.state == 'Kerala')]\n",
        "  return events_data_filtered_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-64xBHbnCtA"
      },
      "source": [
        "def validate_latlong(events_data_filtered_df,events_data_df):\n",
        "  !pip install chart_studio\n",
        "\n",
        "  # Making plotly specific imports\n",
        "  # These imports are necessary to use plotly offline without signing in to their website.\n",
        "  from plotly.offline import init_notebook_mode, iplot\n",
        "  import plotly.graph_objects as go\n",
        "  import chart_studio.plotly as py\n",
        "  from plotly import tools\n",
        "  init_notebook_mode(connected=True)\n",
        "  \n",
        "  import plotly.express as px\n",
        "  fig = px.scatter_mapbox(events_data_filtered_df, lat=\"latitude\", lon=\"longitude\", zoom=3,hover_name='city')\n",
        "  fig.update_layout(mapbox_style=\"open-street-map\")  # <== Using Mapbox\n",
        "  #fig.show()\n",
        "\n",
        "  #checking the number of unique devices for that latitude & longitude\n",
        "  events_data_df[(events_data_df.latitude == 34.555300) & (events_data_df.longitude == 69.207500)]['device_id'].nunique()\n",
        "  events_data_df[(events_data_df.latitude == 25.204800) & (events_data_df.longitude == 55.270800)]['device_id'].nunique()\n",
        "  events_data_df[(events_data_df.latitude == 41.871900) & (events_data_df.longitude == 12.567400)]['device_id'].nunique()\n",
        "\n",
        "  #checking if the device_id is different for the records where latitude & longitude are pointing outside India\n",
        "  dev_id = events_data_df[(((events_data_df.latitude.isnull()) & (events_data_df.longitude.isnull())) | ((events_data_df.latitude == 34.555300) & (events_data_df.longitude == 69.207500)) | ((events_data_df.latitude == 25.204800) & (events_data_df.longitude == 55.270800)) | ((events_data_df.latitude == 41.871900) & (events_data_df.longitude == 12.567400)))].sort_values(by=['device_id'])['device_id'].unique()\n",
        "\n",
        "  #Imputing wrong values in latitude & longitude\n",
        "  #From the previous steps we found that there are 20 device_ids for the wrong values in latitude & longitude records.\n",
        "  #For these 20 device_ids there are unique latitude & longitude combination. Hence, we'll be replacing the missing latitude & longitude values for these devices with the same latitude & longitude values\n",
        "\n",
        "  for i in dev_id:\n",
        "    lat = list(events_data_df[(events_data_df.device_id == i) & (events_data_df.latitude.notnull())].latitude)[0]\n",
        "    lon = list(events_data_df[(events_data_df.device_id == i) & (events_data_df.latitude.notnull())].longitude)[0]\n",
        "    city = list(events_data_df[events_data_df.device_id == i].city)[0]\n",
        "    state = list(events_data_df[events_data_df.device_id == i].state)[0]\n",
        "    #print('For DeviceID {} Latitude is: {}, Longitude is: {}, city is: {} & State is: {}'.format(i,lat,lon,city,state))\n",
        "    events_data_df.loc[events_data_df.device_id == i, 'latitude'] = lat\n",
        "    events_data_df.loc[events_data_df.device_id == i, 'longitude'] = lon\n",
        "\n",
        "  print('\\n Devices with latitude & longitude which are outside India post Imputation are: ',events_data_df[(((events_data_df.latitude == 34.555300) & (events_data_df.longitude == 69.207500)) | ((events_data_df.latitude == 25.204800) & (events_data_df.longitude == 55.270800)) | ((events_data_df.latitude == 41.871900) & (events_data_df.longitude == 12.567400)))]['device_id'].count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuTOtnaZZQRV"
      },
      "source": [
        "def noterun():\n",
        "  #function to load events_data.csv into workspace\n",
        "  load_datasets_from_drive()\n",
        "  #function to read event_data into dataframe, gender_age & mobile_brand tables data into dataframes, get info of all 3 dataframes & check for missing values counts in all 3 dataframes\n",
        "  preprocessing_checks(read_csv_file(), read_dataset_genderage(), read_dataset_phonebrandmodel()) \n",
        "  #function to perform preprocessing on all 3 dataframes wherever applicable to handle missing values\n",
        "  preprocessing(read_csv_file(), read_dataset_phonebrandmodel()) \n",
        "  #function to validate latitude & longitude on dataset which is filtered for 6 states\n",
        "  validate_latlong(filter_data(read_csv_file()),read_csv_file())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AoC_UxA2XPCU",
        "outputId": "acfba157-a3b6-4f1b-c70a-f04b3cb6277c"
      },
      "source": [
        "noterun()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  gdrive/My Drive/CDF_Capstone_Project/events_data.zip\n",
            "replace events_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Requirement already satisfied: mysql-connector-python-rf in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Datasets info is as follows:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3252950 entries, 0 to 3252949\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Dtype  \n",
            "---  ------     -----  \n",
            " 0   event_id   int64  \n",
            " 1   device_id  float64\n",
            " 2   timestamp  object \n",
            " 3   longitude  float64\n",
            " 4   latitude   float64\n",
            " 5   city       object \n",
            " 6   state      object \n",
            "dtypes: float64(3), int64(1), object(3)\n",
            "memory usage: 173.7+ MB\n",
            "\n",
            " Events Data Set info is: None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74645 entries, 0 to 74644\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   device_id  74645 non-null  int64 \n",
            " 1   Gender     74645 non-null  object\n",
            " 2   Age        74645 non-null  int64 \n",
            " 3   Age_Group  74645 non-null  object\n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 2.3+ MB\n",
            "\n",
            " Gender Age Data Set info is:\n",
            " None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 87726 entries, 0 to 87725\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   device_id  87726 non-null  int64 \n",
            " 1   Brand      87726 non-null  object\n",
            " 2   Model      87726 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.0+ MB\n",
            "\n",
            " Mobile Brand Model Data Set info is:\n",
            " None\n",
            "Missing values across 3 datasets are:\n",
            "\n",
            " Missing values in events_data_df is:\n",
            " event_id       0\n",
            "device_id    453\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state        377\n",
            "dtype: int64\n",
            "\n",
            " Missing values in gender_age_train_df is:\n",
            " device_id    0\n",
            "Gender       0\n",
            "Age          0\n",
            "Age_Group    0\n",
            "dtype: int64\n",
            "\n",
            " Missing values in phone_brand_device_model_df is:\n",
            " device_id    0\n",
            "Brand        0\n",
            "Model        0\n",
            "dtype: int64\n",
            "\n",
            " Missing Values Before Imputation are:\n",
            "event_id       0\n",
            "device_id    453\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state        377\n",
            "dtype: int64\n",
            "\n",
            " Missing Values After DeviceID Imputation are:\n",
            "event_id       0\n",
            "device_id      0\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state        377\n",
            "dtype: int64\n",
            "\n",
            " Missing Values After State Imputation are:\n",
            "event_id       0\n",
            "device_id      0\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state          0\n",
            "dtype: int64\n",
            "\n",
            " Missing Values After Latitude & Longitude Imputation are:\n",
            "event_id     0\n",
            "device_id    0\n",
            "timestamp    0\n",
            "longitude    0\n",
            "latitude     0\n",
            "city         0\n",
            "state        0\n",
            "dtype: int64\n",
            "\n",
            " After mapping Non-English Characters in Brand: \n",
            " ['vivo' 'Xiaomi' 'OPPO' 'samsung' 'Coolpad' 'lenovo' 'Huawei' 'Qiku'\n",
            " 'Meizu' 'Phicomm' 'ccmc' 'HTC' 'Ktouch' 'Monkey King' 'LG' 'Opssom'\n",
            " 'youmi' 'ZUK' 'nubia' 'HP' 'nibilu' 'meitu' 'xiangmi' 'moto' 'mengmi'\n",
            " 'hammer' 'fks' 'lshi' 'hisense' 'Hundred Li Feng' 'oneplus' 'yuxin'\n",
            " 'haier' 'CUBE' 'newman' 'bodao' 'dowe' 'lingyun' 'TCL' 'kupo' 'ipair'\n",
            " 'LOGO' 'Cong' 'Taobao' 'asus' 'ada' 'aiyouni' 'Konka' 'yougo' 'banghua'\n",
            " 'UB22' 'heymi' 'Lovme' 'XFPLAY' 'epai' 'hasse' 'nokia' 'pner' 'candy'\n",
            " 'Yitong' 'ouxin' 'micky' 'koobee' 'Ramos' 'smallt' 'Bifer' 'nuomi' 'mige'\n",
            " 'eben' 'ximi' 'Daq' 'Teclast' 'Philips' 'weimi' 'UB7' 'Changhong' 'weitu'\n",
            " 'greeno' 'UB13' 'UB21' 'panasonic' 'UB10' 'Bacardi' 'SUGAR' 'Oukimobile'\n",
            " 'UB1' 'Zhimei' 'obi' 'kiwu' 'fs' 'desci' 'google' 'JXD' 'Kingsun' 'Noain'\n",
            " 'MIL' 'UB20' 'bigcola' 'UB9' 'UB3' 'UB23' 'UB18' 'UB19' 'wpf' 'mole'\n",
            " 'UB24' 'UB4' 'UB5' 'UB17' 'UB2' 'UB12' 'AUX' 'Siemens' 'UB15' 'PPTV']\n",
            "Requirement already satisfied: chart_studio in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart_studio) (4.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart_studio) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2020.12.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Devices having the latitude & longitude which are outside India post Imputation are:  0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}