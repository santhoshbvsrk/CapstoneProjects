{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InsaidTelecom-CDFCapstone_Functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIaPXwWmADiISkCLZU8nxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santhoshbvsrk/CapstoneProjects/blob/main/DataAnalytics_CDF/InsaidTelecom_CDFCapstone_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwbT46AUJXEb"
      },
      "source": [
        "def load_datasets_from_drive():\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount ('/content/gdrive')\n",
        "\n",
        "  !unzip gdrive/My\\ Drive/CDF_Capstone_Project/events_data.zip"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHrx7o7yW3WX"
      },
      "source": [
        "def read_csv_file():\n",
        "  import pandas as pd \n",
        "  import numpy as np\n",
        "\n",
        "  pd.set_option('display.float_format', lambda x: '%18f' %x)\n",
        "\n",
        "  events_data_df=pd.read_csv(\"events_data.csv\")\n",
        "\n",
        "  return events_data_df"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2XfPbnUKyyg"
      },
      "source": [
        "def read_dataset_genderage():\n",
        "\n",
        "  import pandas as pd \n",
        "  import numpy as np\n",
        "\n",
        "  !pip install mysql-connector-python-rf\n",
        "\n",
        "  import mysql.connector\n",
        "  \n",
        "#this is to connect to mysql database\n",
        "  mydb = mysql.connector.connect(\n",
        "      host=\"cpanel.insaid.co\",\n",
        "      user=\"student\",\n",
        "      password=\"student\",\n",
        "      database=\"Capstone1\"\n",
        "    )\n",
        "\n",
        "#declaring a cursor variable of database to hold data of a specific table\n",
        "  mycursor = mydb.cursor()\n",
        "\n",
        "#mycursor variable holds the entire data from gender_age_train table\n",
        "  mycursor.execute(\"SELECT * FROM gender_age_train\")\n",
        "\n",
        "#using fetchall function to get data from cursor and populating in another variable\n",
        "  myresult = mycursor.fetchall()\n",
        "\n",
        "#inserting table data from variable into a dataframe\n",
        "  gender_age_train_df = pd.DataFrame(myresult,columns=['device_id','Gender','Age','Age_Group'])\n",
        "  return gender_age_train_df"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKf_tEgUewWT"
      },
      "source": [
        "def read_dataset_phonebrandmodel():\n",
        "\n",
        "  import pandas as pd \n",
        "  import numpy as np\n",
        "  import mysql.connector\n",
        "  \n",
        "#this is to connect to mysql database\n",
        "  mydb = mysql.connector.connect(\n",
        "      host=\"cpanel.insaid.co\",\n",
        "      user=\"student\",\n",
        "      password=\"student\",\n",
        "      database=\"Capstone1\"\n",
        "    )\n",
        "\n",
        "#declaring a cursor variable of database to hold data of a specific table\n",
        "  mycursor = mydb.cursor()\n",
        "\n",
        "#mycursor variable holds the entire data from phone_brand_device_model table\n",
        "  mycursor.execute(\"SELECT * FROM phone_brand_device_model\")\n",
        "\n",
        "#using fetchall function to get data from cursor and populating in another variable, this overwrites data from previous load\n",
        "  myresult = mycursor.fetchall()\n",
        "\n",
        "#inserting table data from variable into a dataframe\n",
        "  phone_brand_device_model_df = pd.DataFrame(myresult,columns=['device_id','Brand','Model'])\n",
        "  return phone_brand_device_model_df"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Di18-WFdEPP"
      },
      "source": [
        "def preprocessing_checks(events_data_df,gender_age_train_df,phone_brand_device_model_df):\n",
        "  print(\"Missing values across 3 datasets are:\")  \n",
        "  return events_data_df.isnull().sum(),gender_age_train_df.isnull().sum(),phone_brand_device_model_df.isnull().sum() "
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-FlECGOJV2U",
        "outputId": "254bd725-2842-46fb-ffb5-36e41c8e8276"
      },
      "source": [
        "load_datasets_from_drive()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  gdrive/My Drive/CDF_Capstone_Project/events_data.zip\n",
            "replace events_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: events_data.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_K95b5bdVbh",
        "outputId": "8b973527-f095-42a4-bbc0-1912736a123c"
      },
      "source": [
        "preprocessing_checks(read_csv_file(), read_dataset_genderage(), read_dataset_phonebrandmodel())"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mysql-connector-python-rf in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Missing values across 3 datasets are:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(event_id       0\n",
              " device_id    453\n",
              " timestamp      0\n",
              " longitude    423\n",
              " latitude     423\n",
              " city           0\n",
              " state        377\n",
              " dtype: int64, device_id    0\n",
              " Gender       0\n",
              " Age          0\n",
              " Age_Group    0\n",
              " dtype: int64, device_id    0\n",
              " Brand        0\n",
              " Model        0\n",
              " dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0tNm9zWiWBF"
      },
      "source": [
        "def preprocessing(events_data_df,phone_brand_device_model_df):\n",
        "  print(\"Missing Values Before Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #Preparing list of unique latitude values for the missing deviceid records.\n",
        "  latlon_df = events_data_df[events_data_df.device_id.isnull()]['latitude'].unique()\n",
        "  #For imputing Missing values in DeviceId, we've fetched unique latitude for the missing deviceid and found that for that latitude there is only one deviceid throughout the dataset.\n",
        "  #Hence, we'll be imputing missing deviceid values by replacing the deviceid which the same latitudeid shares\n",
        "  for latitude in latlon_df:\n",
        "    devid = list(events_data_df[(events_data_df.latitude == latitude) & (events_data_df.device_id.notnull())].device_id)[0]\n",
        "    #print('Latitude is {} for DeviceID: {}'.format(devid,latitude))\n",
        "    events_data_df.loc[events_data_df.latitude == latitude, 'device_id'] = devid\n",
        "  #Missing Values post imputing DeviceID\n",
        "  print(\"Missing Values After DeviceID Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #For imputing the missing values in State, we'll first find all their respecitve city values\n",
        "  city_list = list(events_data_df[events_data_df.state.isnull()].city.unique())\n",
        "  #Will loop over the above cities list, fetch it's relevant state value where state value is not null and then populate that to all the records where city value is matching with the city value from our list\n",
        "  for city in city_list:\n",
        "    state = list(events_data_df[(events_data_df.city == city) & (events_data_df.state.notnull())].state)[0]\n",
        "    #print(\"State is {} for City {}\".format(state,city))\n",
        "    events_data_df.loc[events_data_df.city == city, 'state'] = state\n",
        "  #Missing Values post imputing State\n",
        "  print(\"Missing Values After State Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #checking if the device_id is different for the records where latitude & longitude are null\n",
        "  dev_id = events_data_df[(events_data_df.latitude.isnull()) & (events_data_df.longitude.isnull())].sort_values(by=['device_id'])['device_id'].unique()\n",
        "  #Imputing missing values in latitude & longitude\n",
        "  #From the previous steps we found that there are 21 device_ids for the missing latitude & longitude records.\n",
        "  #For these 21 device_ids there are unique latitude & longitude combination. Hence, we'll be replacing the missing latitude & longitude values for these devices with the same latitude & longitude values\n",
        "  for i in dev_id:\n",
        "    lat = list(events_data_df[(events_data_df.device_id == i) & (events_data_df.latitude.notnull())].latitude)[0]\n",
        "    lon = list(events_data_df[(events_data_df.device_id == i) & (events_data_df.latitude.notnull())].longitude)[0]\n",
        "    city = list(events_data_df[events_data_df.device_id == i].city)[0]\n",
        "    state = list(events_data_df[events_data_df.device_id == i].state)[0]\n",
        "    #print('For DeviceID {} Latitude is: {}, Longitude is: {}, city is: {} & State is: {}'.format(i,lat,lon,city,state))\n",
        "    events_data_df.loc[events_data_df.device_id == i, 'latitude'] = lat\n",
        "    events_data_df.loc[events_data_df.device_id == i, 'longitude'] = lon\n",
        " #Missing Values post imputing Latitude & Longitude\n",
        "  print(\"Missing Values After Latitude & Longitude Imputation are:\")\n",
        "  print(events_data_df.isnull().sum())\n",
        "\n",
        "  #To replace Non-English Characters in Brand column of phone_brand_device_model dataframe\n",
        "  #Add a brand name per brand in Chinese for which we don´t have a translation\n",
        "  phone_brands_mapping = {\"三星\": \"samsung\",\"天语\": \"Ktouch\", \"海信\": \"hisense\", \"联想\": \"lenovo\", \"欧比\": \"obi\",\n",
        "                                \"爱派尔\": \"ipair\", \"努比亚\": \"nubia\", \"优米\": \"youmi\", \"朵唯\": \"dowe\", \"黑米\": \"heymi\",\n",
        "                                \"锤子\": \"hammer\", \"酷比魔方\": \"koobee\", \"美图\": \"meitu\", \"尼比鲁\": \"nibilu\", \"一加\": \"oneplus\",\n",
        "                                \"优购\": \"yougo\", \"诺基亚\": \"nokia\", \"糖葫芦\": \"candy\", \"中国移动\": \"ccmc\", \"语信\": \"yuxin\",\n",
        "                                \"基伍\": \"kiwu\", \"青橙\": \"greeno\", \"华硕\": \"asus\", \"夏新\": \"panasonic\", \"维图\": \"weitu\",\n",
        "                                \"艾优尼\": \"aiyouni\", \"摩托罗拉\": \"moto\", \"乡米\": \"xiangmi\", \"米奇\": \"micky\", \"大可乐\": \"bigcola\",\n",
        "                                \"沃普丰\": \"wpf\", \"神舟\": \"hasse\", \"摩乐\": \"mole\", \"飞秒\": \"fs\", \"米歌\": \"mige\", \"富可视\": \"fks\",\n",
        "                                \"德赛\": \"desci\", \"梦米\": \"mengmi\", \"乐视\": \"lshi\", \"小杨树\": \"smallt\", \"纽曼\": \"newman\",\n",
        "                                \"邦华\": \"banghua\", \"E派\": \"epai\", \"易派\": \"epai\", \"普耐尔\": \"pner\", \"欧新\": \"ouxin\", \"西米\": \"ximi\",\n",
        "                                \"海尔\": \"haier\", \"波导\": \"bodao\", \"糯米\": \"nuomi\", \"唯米\": \"weimi\", \"酷珀\": \"kupo\", \"谷歌\": \"google\",\n",
        "                                \"昂达\": \"ada\", \"聆韵\": \"lingyun\", \"小米\": \"Xiaomi\", \"华为\": \"Huawei\", \"魅族\": \"Meizu\", \"中兴\": \"ZTE\",\n",
        "                                \"酷派\": \"Coolpad\", \"金立\": \"Gionee\", \"SUGAR\": \"SUGAR\", \"OPPO\": \"OPPO\", \"vivo\": \"vivo\", \"HTC\": \"HTC\",\n",
        "                                \"LG\": \"LG\", \"ZUK\": \"ZUK\", \"TCL\": \"TCL\", \"LOGO\": \"LOGO\", \"SUGAR\": \"SUGAR\", \"Lovme\": \"Lovme\",\n",
        "                                \"PPTV\": \"PPTV\", \"ZOYE\": \"ZOYE\", \"MIL\": \"MIL\", \"索尼\" : \"Sony\", \"欧博信\" : \"Opssom\", \"奇酷\" : \"Qiku\",\n",
        "                                \"酷比\" : \"CUBE\", \"康佳\" : \"Konka\", \"亿通\" : \"Yitong\", \"金星数码\" : \"JXD\", \"至尊宝\" : \"Monkey King\",\n",
        "                                \"百立丰\" : \"Hundred Li Feng\", \"贝尔丰\" : \"Bifer\", \"百加\" : \"Bacardi\", \"诺亚信\" : \"Noain\", \n",
        "                                \"广信\" : \"Kingsun\", \"世纪天元\" : \"Ctyon\", \"青葱\" : \"Cong\", \"果米\" : \"Taobao\", \"斐讯\" : \"Phicomm\",\n",
        "                                \"长虹\" : \"Changhong\", \"欧奇\" : \"Oukimobile\", \"先锋\" : \"XFPLAY\", \"台电\" : \"Teclast\", \"大Q\" : \"Daq\",\n",
        "                                \"蓝魔\" : \"Ramos\", \"奥克斯\" : \"AUX\", \"索尼\" : \"Sony\", \"欧博信\" : \"Opssom\", \"奇酷\" : \"Qiku\",\n",
        "                                \"酷比\" : \"CUBE\", \"康佳\" : \"Konka\", \"亿通\" : \"Yitong\", \"金星数码\" : \"JXD\", \"至尊宝\" : \"Monkey King\",\n",
        "                                \"百立丰\" : \"Hundred Li Feng\", \"贝尔丰\" : \"Bifer\", \"百加\" : \"Bacardi\", \"诺亚信\" : \"Noain\",\n",
        "                                \"广信\" : \"Kingsun\", \"世纪天元\" : \"Ctyon\", \"青葱\" : \"Cong\", \"果米\" : \"Taobao\", \"斐讯\" : \"Phicomm\",\n",
        "                                \"长虹\" : \"Changhong\", \"欧奇\" : \"Oukimobile\", \"先锋\" : \"XFPLAY\", \"台电\" : \"Teclast\", \"大Q\" : \"Daq\", \n",
        "                                \"蓝魔\" : \"Ramos\", \"奥克斯\" : \"AUX\", \"飞利浦\": \"Philips\", \"智镁\": \"Zhimei\", \"惠普\": \"HP\",\n",
        "                                \"原点\": \"Origin\", \"戴尔\": \"Dell\", \"碟米\": \"Diemi\", \"西门子\": \"Siemens\", \"亚马逊\": \"Amazon\",\n",
        "                                \"宏碁\": \"Acer\",\n",
        "                                '世纪星': \"UB1\", '丰米': \"UB2\", '优语':'UB3', '凯利通': \"UB4\", '唯比': \"UB5\", '嘉源': \"UB6\",\n",
        "                                 '大显': \"UB7\", '天宏时代': \"UB8\", '宝捷讯': 'UB9','帷幄': 'UB10', '德卡诺': 'UB11',\n",
        "                                '恒宇丰': 'UB12', '本为': 'UB13', '极米': 'UB14', '欧乐迪': 'UB15', '欧乐酷': 'UB16',\n",
        "                                '欧沃': 'UB17', '瑞米': 'UB18', '瑞高': 'UB19', '白米': 'UB20', '虾米': 'UB21', '赛博宇华': 'UB22',\n",
        "                                '首云': 'UB23', '鲜米': 'UB24'}\n",
        "\n",
        "  phone_brand_device_model_df.Brand = phone_brand_device_model_df.Brand.apply(lambda x: phone_brands_mapping.setdefault(x,x))\n",
        "  print(\"\\n After mapping Non-English Characters in Brand: \\n\",phone_brand_device_model_df.Brand.unique())\n",
        "  "
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJR4glAOisNM",
        "outputId": "d6e3b390-0912-488e-8ca4-80770d4c0b7e"
      },
      "source": [
        "preprocessing(read_csv_file(), read_dataset_phonebrandmodel())"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing Values Before Imputation are:\n",
            "event_id       0\n",
            "device_id    453\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state        377\n",
            "dtype: int64\n",
            "Missing Values After DeviceID Imputation are:\n",
            "event_id       0\n",
            "device_id      0\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state        377\n",
            "dtype: int64\n",
            "Missing Values After State Imputation are:\n",
            "event_id       0\n",
            "device_id      0\n",
            "timestamp      0\n",
            "longitude    423\n",
            "latitude     423\n",
            "city           0\n",
            "state          0\n",
            "dtype: int64\n",
            "Missing Values After Latitude & Longitude Imputation are:\n",
            "event_id     0\n",
            "device_id    0\n",
            "timestamp    0\n",
            "longitude    0\n",
            "latitude     0\n",
            "city         0\n",
            "state        0\n",
            "dtype: int64\n",
            "After mapping Non-English Characters in Brand: \n",
            " ['vivo' 'Xiaomi' 'OPPO' 'samsung' 'Coolpad' '联想 ' 'Huawei' 'Qiku' 'Meizu'\n",
            " 'Phicomm' 'ccmc' 'HTC' 'Ktouch' 'Monkey King' 'LG' 'Opssom' 'youmi' 'ZUK'\n",
            " 'nubia' 'HP' 'nibilu' 'meitu' 'xiangmi' 'moto' 'mengmi' 'hammer' 'fks'\n",
            " 'lshi' 'hisense' 'Hundred Li Feng' 'oneplus' 'yuxin' 'haier' 'CUBE'\n",
            " 'newman' 'bodao' 'dowe' 'lingyun' 'TCL' 'kupo' 'ipair' 'LOGO' 'Cong'\n",
            " 'Taobao' 'asus' 'ada' 'aiyouni' 'Konka' 'yougo' 'banghua' 'UB22' 'heymi'\n",
            " 'Lovme' 'XFPLAY' 'epai' 'hasse' 'nokia' 'pner' 'candy' 'Yitong' 'ouxin'\n",
            " 'micky' 'koobee' 'Ramos' 'smallt' 'Bifer' 'nuomi' 'mige' 'E人E本' 'ximi'\n",
            " 'Daq' 'Teclast' 'Philips' 'weimi' 'UB7' 'Changhong' 'weitu' 'greeno'\n",
            " 'UB13' 'UB21' 'panasonic' 'UB10' 'Bacardi' 'SUGAR' 'Oukimobile' 'UB1'\n",
            " 'Zhimei' 'obi' 'kiwu' 'fs' 'desci' 'google' 'JXD' 'Kingsun' 'Noain' 'MIL'\n",
            " 'UB20' 'bigcola' 'UB9' 'UB3' 'UB23' 'UB18' 'UB19' 'wpf' 'mole' 'UB24'\n",
            " 'UB4' 'UB5' 'UB17' 'UB2' 'UB12' 'AUX' 'Siemens' 'UB15' 'PPTV']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Tfz4RNnkgU"
      },
      "source": [
        "def filter_data(events_data_df):\n",
        "  #creating a dataframe which contains event data for only 6 states under consideration\n",
        "  events_data_filtered_df= events_data_df[(events_data_df.state == 'WestBengal') | (events_data_df.state == 'Karnataka') | (events_data_df.state == 'Gujarat') | (events_data_df.state == 'Bihar') | (events_data_df.state == 'Punjab') | (events_data_df.state == 'Kerala')]\n",
        "  return events_data_filtered_df"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-64xBHbnCtA"
      },
      "source": [
        "def validate_latlong(events_data_filtered_df):\n",
        "  !pip install chart_studio\n",
        "\n",
        "  # Making plotly specific imports\n",
        "  # These imports are necessary to use plotly offline without signing in to their website.\n",
        "  from plotly.offline import init_notebook_mode, iplot\n",
        "  import plotly.graph_objects as go\n",
        "  import chart_studio.plotly as py\n",
        "  from plotly import tools\n",
        "  init_notebook_mode(connected=True)\n",
        "  \n",
        "  import plotly.express as px\n",
        "  fig = px.scatter_mapbox(events_data_filtered_df, lat=\"latitude\", lon=\"longitude\", zoom=3,hover_name='city')\n",
        "  fig.update_layout(mapbox_style=\"open-street-map\")  # <== Using Mapbox\n",
        "  #fig.show()"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "0C1E7r_CnzQD",
        "outputId": "622e6c49-45af-4730-e101-1fad293c5af8"
      },
      "source": [
        "validate_latlong(filter_data(read_csv_file()))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chart_studio in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.3.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart_studio) (4.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart_studio) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (1.24.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}